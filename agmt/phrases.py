import psycopg2, re
import spacy
from spacy.matcher import Matcher
from gensim.models.phrases import Phrases

# the puctuations that are removed from text for getting a clean text
# "-" is left out intentionally in this list because, it is ofter used in text to show compund words
non_letters = [',', '"', '!', '.', '\n', '\\','“','”','“','*','।','?',';',"'","’","(",")","‘","—"]
non_letter_pattern = re.compile(r'['+''.join(non_letters)+']')
multi_space_pattern = re.compile(r'\s\s+')

# the stop words are functional words in Hindi, which are treated separately while generating phrases
hi_stop_words = [ "ओर", "कर", "करके", "करता", "करते", "करना", "करने", "करे", "करें", "करेगा", "करो", 
			  "का", "कि", "किया", "किस", "किसी", "की", "के", "को",  "तो", "था", "थी", "थे", "ने", "पर"
			  "भी", "में", "रहा", "रहे", "रहो", "से", "हर", "ही", "हुआ", "हुई", "हुए", "हुओं", "हूँ", "हे"
			  "है", "हैं", "हो", "होकर", "होगा", "होता", "होने" ]


# the scoring algo used to score the phrases generated by rules and naive-freq models
def phrase_rank(phrase_freq,word_freqs):
	score = phrase_freq*len(word_freqs)*100
	for f in word_freqs:
		score /= (f/10)
	return score

# retruns a freq dictionary of words in the input snetences
# used for scoring the phrases generated by n-gram and rules
def uniquewords_freq_dict(sent_stream):
	word_list = {}

	for verse in sent_stream:
		for w in verse:
			if w in word_list:
				word_list[w] += 1
			else:
				word_list[w] = 1
	sorted_word_list = {k:word_list[k] for k in sorted(word_list,key=word_list.get,reverse=True)}
	return sorted_word_list

# removes punctuations and splits the sentence into words
def cleanNsplit(sent):
	try:
		sent = re.sub(non_letter_pattern," ",sent)
		sent = re.sub(multi_space_pattern," ",sent)
		sent = sent.strip()
		sent = sent.split(' ')
		
	except Exception as e:
		print('sent:',sent)
		raise e
	return sent


#############Using Gensim #######################

def train_bigram_gensimmodel(sentence_stream,stop_words):
	bigram_phrase_model = Phrases(sentence_stream, common_terms=frozenset(stop_words), min_count=5	, threshold=10)
	return bigram_phrase_model
def train_trigram_gensimmodel(sentence_stream,stop_words):
	bigram_phrase_model = train_bigram_gensimmodel(sentence_stream,stop_words)
	trigram_phrase_model = Phrases(bigram_phrase_model[sentence_stream], common_terms=frozenset(stop_words), min_count=3, threshold=10)
	return trigram_phrase_model
def gensimphrases_dict(model,sentence_stream):
	phrase_list = {}
	for phrase, score in model.export_phrases(sentence_stream):
		if phrase not in phrase_list:
			phrase = phrase.decode("utf-8")
			phrase_list[phrase] = {'freq' : 1, 'score':score}
		else:
			phrase_list[phrase]['freq'] += 1
	return phrase_list    

# Pulls bible text from DB
# returns a dict of phrases, with freq and score 
# uses the Phrases module of gensim library
def extract_phrases_gensim(conn,lang,version):
	source_table = lang+'_'+version+'_bible_cleaned'

	cursor = conn.cursor()
	cursor.execute("select ref_id, verse from " + source_table + " order by ref_id;")
	verses = cursor.fetchall()
	text = [cleanNsplit(v[1])for v in verses]

	if lang == "hi" or lang == "hin":
		stop_words = hi_stop_words
	else:
		stop_words = []

	model = train_trigram_gensimmodel(text,stop_words)
	phrases = gensimphrases_dict(model,text)
	return phrases

########################################################



###########Naive N-gram Model ###########################

def get_bigrams(sent):
	bigrams = []
	for i in range(len(sent)-1):
		bigrams.append((sent[i],sent[i+1]))
	return bigrams  
def get_trigrams(sent):
	trigrams = []
	for i in range(len(sent)-2):
		trigrams.append((sent[i],sent[i+1],sent[i+2]))
	return trigrams  
def ngramphrases_dict(sent_stream, word_dict):
	phrase_list = {}
	for verse in sent_stream:
		bigrams_in_verse = get_bigrams(verse)
		trigrams_in_verse = get_trigrams(verse)
		for bigram in bigrams_in_verse:
			if bigram in phrase_list:
				phrase_list[bigram] +=1
			else:
				phrase_list[bigram] = 1
		for trigram in trigrams_in_verse:
			if trigram in phrase_list:
				phrase_list[trigram] +=1
			else:
				phrase_list[trigram] = 1
	sorted_phrase_list = {k: phrase_list[k] for k in sorted(phrase_list, key=phrase_list.get, reverse=True)}

	phrase_score_dict = {" ".join(list(ph)):{'freq':phrase_list[ph],'score':phrase_rank(phrase_list[ph],[word_dict[w] for w in ph])} for ph in phrase_list}
	return phrase_score_dict

# pulls all available bible text from DB
# returns a dict of phrases, with freq and score 
# all bi-grams and tri-grams are considered as valid phrases
def extract_phrases_naivestat(conn,lang,version):
	source_table = lang+'_'+version+'_bible_cleaned'

	cursor = conn.cursor()
	cursor.execute("select ref_id, verse from " + source_table + " order by ref_id;")
	verses = cursor.fetchall()
	text = [cleanNsplit(v[1])for v in verses]

	word_dict = uniquewords_freq_dict(text)
	phrases = ngramphrases_dict(text,word_dict)
	return phrases


###########################################################


################### Using Spacy ###########################

# reads rules from a  file text
# format of rules file is as follows
#		[{"DEP":"case"},{"DEP":"case"}]
#		[{"DEP":"cc"},{"TAG":"NN"},{"DEP":"case"}]
#		[{"DEP":"cc"},{"TAG":"NN"},{"DEP":"cc"},{"TAG":"VM"}]
#		[{"DEP":"cc"},{"TAG":"NNP"}]
# Add them to DB
def add_rules_toDB(conn,lang,input_file):
	rules_table = lang+'_phrase_rules'
	cursor = conn.cursor()

	cursor.execute("select exists (select * from information_schema.tables where table_name= '" + rules_table + "')")
	tableExists = cursor.fetchone()[0]

	print('checking table')
	if not tableExists:
		cursor.execute("CREATE TABLE "+rules_table+"(ID INT NOT NUll, Rule TEXT NOT NULL)")
		conn.commit()
	else:
		print('table found')
		cursor.execute("DELETE FROM "+rules_table+";")
		print('truncated '+rules_table)
		conn.commit()

	with open(input_file,'r') as infile:
		for i,line in enumerate(infile):
			cursor.execute("INSERT INTO "+rules_table+" VALUES(%s,%s)",(i,line))
		conn.commit()

def get_spacyphrases(verse,nlp,matcher):
	doc = nlp(verse)
	matches = matcher(doc)
	phrases = []
	for match in matches:
		phrases.append(doc[match[1]:match[2]].text)
	return phrases

def spacyphrases_dict(sent_stream,nlp,matcher,word_dict):
	phrase_list = {}
	# for verse in sent_stream:
		# phrases_in_verse = get_spacyphrases(verse,nlp,matcher)
		# for phrase in phrases_in_verse:
	batches = [ ]
	for x in range(int(len(sent_stream)/5000)-1):
		batches.append(" ".join(sent_stream[x*5000:x*5000+5000]))
	batches.append(" ".join(sent_stream[-5000:]))
	for batch in batches:
		phrases_in_batch = get_spacyphrases(batch,nlp,matcher)
		for phrase in phrases_in_batch:
			if phrase in phrase_list:
				phrase_list[phrase] +=1
			else:
				phrase_list[phrase] = 1
	print('obtained phrases... now sorting and scoring them')
	sorted_phrase_list = {k: phrase_list[k] for k in sorted(phrase_list, key=phrase_list.get, reverse=True)}
	try:
		phrase_score_dict = {ph:{
						'freq':sorted_phrase_list[ph],
						'score':phrase_rank(sorted_phrase_list[ph],[word_dict[w] for w in ph.split(" ")])} 
						for ph in sorted_phrase_list }
		# pass
	except Exception as e:
		# print(sorted_phrase_list:,sorted_phrase_list)
		print('ph:',ph)
		raise e
	return phrase_score_dict

# pulls all available bible text from DB
# returns a dict of phrases, with freq and score 
# uses the matcher class of spacy rather than the phrases class
def extract_phrases_rulebased(conn,lang,version,start=None,end=None):
	source_table = lang+'_'+version+'_bible_cleaned'
	rules_table = lang+'_phrase_rules'

	cursor = conn.cursor()

	cursor.execute("select exists (select * from information_schema.tables where table_name= '" + rules_table + "')")
	tableExists = cursor.fetchone()[0]

	if not tableExists:
		print("!!!No Rules found in DB! Falls back to Gensim tokenizer")
		phrases = extract_phrases_gensim(conn,lang,version)
	else:
		nlp = spacy.load('models/model-final')
		matcher = Matcher(nlp.vocab)

		cursor.execute("SELECT ID,Rule from "+rules_table+" order by ID;")
		rules = cursor.fetchall()

		for row in rules:
			rul = eval(row[1])
			matcher.add('rule'+str(row[0]),None,rul)

		if start and end:
			cursor.execute("select ref_id, verse from " + source_table + " where ref_id>="+str(start)+" and ref_id<"+str(end)+" order by ref_id;")
		elif start and not end:
			end = start + 1000000
			cursor.execute("select ref_id, verse from " + source_table + " where ref_id>="+str(start)+" and ref_id<"+str(end)+" order by ref_id;")
		else:
			cursor.execute("select ref_id, verse from " + source_table + " order by ref_id;")
		verses = cursor.fetchall()
		text = [' '.join(cleanNsplit(v[1])) for v in verses]
		word_split_text = [cleanNsplit(v[1]) for v in verses]

		word_dict = uniquewords_freq_dict(word_split_text)
		phrases = spacyphrases_dict(text,nlp,matcher,word_dict)
	return phrases

#################################################################



# The method can identify phrases from all the available text for the specified lang and version
# Then generate the tokens(if possible phrases, other wise words)
# for the selected Book.
# the generated tokens are populated into the DB table for that lnaguage
# Example usage from an extenral file:
#		import phrases
#		import psycopg2
#
#		db = psycopg2.connect(dbname='mt2414_local', user='postgres', password='password', host='localhost', port=5432)
#		phrases.tokenize(conn=db, lang='hi', version='irv4', book_id=40)
# it can take an optional parameter algo
# algo takes values `gensim`, `ngram`, `gensim-ngram`, `rule-based` or `single-word`. If not specified, defaults to `gensim-ngram`	
def tokenize(conn,lang,version,book_id,algo='gensim-ngram'):
	start_refid = book_id * 1000000
	end_refid = start_refid + 1000000
	if (algo == 'gensim'):
		phrases = extract_phrases_gensim(conn,lang,version)
	elif( algo == 'ngram'):
		phrases = extract_phrases_naivestat(conn,lang,version)
		# print(phrases.keys())
		# return
	elif( algo == 'rule-based'):
		phrases = extract_phrases_rulebased(conn,lang,version,start=start_refid,end=end_refid)
	elif( algo == 'single-word'):
		phrases = {}
	elif ( algo == 'gensim-ngram'):
		phrases = extract_phrases_gensim(conn,lang,version)
		phrases2 = extract_phrases_naivestat(conn,lang,version)
		for i,ph in enumerate(phrases2):
			if ph not in phrases:
				phrases[ph] = phrases2[ph]
			if i > 250:
				break
	
	stop_words = []
	if lang == 'hi' or lang == 'hin':
		stop_words = hi_stop_words

	cursor = conn.cursor()
	tw_table = lang+'_tw'
	cursor.execute("select exists (select * from information_schema.tables where table_name= '" + tw_table + "')")
	tableExists = cursor.fetchone()[0]
	if tableExists:
		cursor.execute('select wordforms from '+tw_table+' order by id;')
	tws = []
	for row in cursor.fetchall():
		wordforms = [x.strip() for x in row[0].split(',')]
		tws += wordforms
	
	for ph in tws:
		if ph not in phrases:
			phrases[ph] = {'freq':None,'score':None}

	source_table = lang+'_'+version+'_bible_cleaned'
	if start_refid and end_refid:
		query = "select ref_id, verse from " + source_table + " where ref_id>="+str(start_refid)+" and ref_id<"+str(end_refid)+" order by ref_id;"
	else:
		query = "select ref_id, verse from " + source_table + " order by ref_id;"
	cursor.execute(query)
	rows = cursor.fetchall()
	verses = [(r[0],cleanNsplit(r[1])) for r in rows]

	token_table =lang+'_'+version+'_bible_tokens'
	cursor.execute("select exists (select * from information_schema.tables where table_name= '" + token_table + "')")
	tableExists = cursor.fetchone()[0]

	if not tableExists:
		cursor.execute("CREATE TABLE "+token_table+"(book_id INT NOT NUll, token TEXT NOT NULL)")
		conn.commit()
	else:
		# in the assumption that tokenization would always be done for one book at a time
		cursor.execute("DELETE FROM "+token_table+" WHERE book_id="+str(book_id)+" ;")
		conn.commit()		

	tokens = []
	for row in verses:
		ref_id = row[0]
		word_split_text = row[1]
		N = len(word_split_text)
		taken = [False for i in range(N)]
		for n in range(N,1,-1):
			for i in range(N-n+1):
				chunk = word_split_text[i:i+n]
				if ' '.join(chunk) in phrases:
					taken_check = False
					for index in range(i,i+n):
						if taken[index]:
							taken_check = True
							break
					if taken_check == False:
						for index in range(i,i+n):
							taken[index] = True
						phrase = ' '.join(word_split_text[i:i+n])
						if phrase not in tokens: 
							tokens.append(phrase)
		for i,flag in enumerate(taken):
			if not flag:
				word_token = word_split_text[i]
				if word_token not in stop_words and word_token not in tokens:
					tokens.append(word_token)

	tokens = sorted(tokens)
	for tok in tokens:
		cursor.execute("INSERT INTO "+token_table+" (book_id, token) VALUES(%s,%s)",(book_id,tok))
		# print(tok)
	conn.commit()
	cursor.close()



##############################################################################






################################ Draft Genaration ############################

tokenTranslatedDict = {}

def loadPhraseTranslations(conn, sourceId, targetLanguageId):
	global tokenTranslatedDict
	cursor = conn.cursor()
	cursor.execute("select token, translation from translations where source_id=%s \
        and target_id=%s", (sourceId, targetLanguageId))
	rst = cursor.fetchall()
	if rst:
		tokenTranslatedDict = {k:v for k,v in rst}
		return True
	else:
		print("!!!!Error: token translations not obtained!!!")
		return False


def getNgrams(sent,n):
	ngrams = []
	for i in range(len(sent)):
		if i+n <= len(sent)+1:
			ngrams.append(sent[i:i+n-1])
	return ngrams



def translateText(text_snippet):
	words_in_text = re.split(r"\s",text_snippet)


	taken = [0 for word in words_in_text]
	translation = ['NULL' for word in words_in_text]

	N = len(words_in_text)
	for n in range(N,1,-1):
		nPhrases = getNgrams(words_in_text, n)
		for i,phrase in enumerate(nPhrases):
			not_taken = True
			phrase_text = " ".join(phrase)
			for pos in range(i,i+len(phrase)):
				if taken[pos] == 1:
					not_taken = False
					break
			if not_taken and phrase_text in tokenTranslatedDict:
				translated_phrase = tokenTranslatedDict[phrase_text]
				translation[i] = translated_phrase
				taken[i] = 1
				for pos in range(i+1,i+len(phrase)):
					translation[pos] = ''
					taken[pos] = 1
	for index,value in enumerate(taken):
		if value == 0:
			translation[index] = words_in_text[index]
	translation = " ".join(translation)
	return translation





if __name__ == '__main__':
	db = psycopg2.connect(dbname='mt2414_local', user='postgres', password='password', host='localhost', port=5432)

	# tokenize(db,'hi','irv4',40,algo='gensim')
	# tokenize(db,'hi','irv4',40,algo='ngram')
	# tokenize(db,'hi','irv4',40,algo='rule-based')
	# tokenize(db,'hi','irv4',40,algo='single-word')
	# tokenize(db,'hi','irv4',40,algo='gensim-ngram')

	# add_rules_toDB(db,"hi","rules_to_DB_draft2.txt")


	loadPhraseTranslations(db,1,2)

	print(translateText('1 3 2 1 2 5 4 0 5'))
	print(translateText(' '))
	print(translateText('   '))
	db.close()